{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter(\"ignore\")\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import kfp.components as comp\n",
    "features_path='data/training.parquet'\n",
    "train_image='gcr.io/stobias-dev/spam-trainer-image:latest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_upload():\n",
    "    import s3fs\n",
    "    s3_endpoint='https://minio-kubeflow.apps.kubeflow.openshift.red8.cloud'\n",
    "    access_key='minio'\n",
    "    secret_access_key='minio123'\n",
    "    bucket_name='mlpipeline'\n",
    "    region='us-east-1'\n",
    "    s3 = s3fs.S3FileSystem(\n",
    "    client_kwargs=dict(\n",
    "        endpoint_url=s3_endpoint),\n",
    "        key=access_key,\n",
    "        secret=secret_access_key)\n",
    "    \n",
    "def get_s3_client():\n",
    "    import s3fs\n",
    "    import pyarrow.parquet as pq\n",
    "    s3_endpoint='https://minio-kubeflow.apps.kubeflow.openshift.red8.cloud'\n",
    "    access_key='minio'\n",
    "    secret_access_key='minio123'\n",
    "    bucket_name='mlpipeline'\n",
    "    region='us-east-1'\n",
    "    \n",
    "    return s3fs.S3FileSystem(\n",
    "        client_kwargs=dict(\n",
    "            endpoint_url=s3_endpoint\n",
    "        ),\n",
    "        key=access_key,\n",
    "        secret=secret_access_key\n",
    "        )\n",
    "def create_training_data_vol(namespace,model_name,storage_class):\n",
    "    return dsl.VolumeOp(\n",
    "        name='model_volume',\n",
    "        resource_name=str(model_name) + '-modelpvc',\n",
    "        size='10Gi',\n",
    "        modes=['ReadWriteMany'],\n",
    "        storage_class=storage_class\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create feature file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_features(bucket_name,training_data_path,features_output_path):\n",
    "    import s3fs\n",
    "    import pyarrow.parquet as pq\n",
    "    from sklearn.feature_extraction.text import HashingVectorizer,TfidfTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    import pickle, os\n",
    "    from mlworkflows import util\n",
    "    \n",
    "    s3 = get_s3_client()\n",
    "    # Download the dataset.\n",
    "    pandas_dataframe = pq.ParquetDataset(f\"s3://{bucket_name}/{training_data_path}\", filesystem=s3).read_pandas().to_pandas()\n",
    "    vect = HashingVectorizer(norm=None, token_pattern='(?u)\\\\b[A-Za-z]\\\\w+\\\\b', n_features=1024, alternate_sign = False)\n",
    "    tfidf = TfidfTransformer()\n",
    "    feat_pipeline = Pipeline([\n",
    "        ('vect',vect),\n",
    "        ('tfidf',tfidf)\n",
    "    ])\n",
    "    ## Save our vectors\n",
    "    util.serialize_to(feat_pipeline, \"feature_pipeline.sav\")\n",
    "    feature_vecs = feat_pipeline.fit_transform(data[\"text\"]).toarray()\n",
    "    labeled_vecs = pd.concat([data.reset_index()[[\"index\", \"label\"]],\n",
    "                                    pd.DataFrame(feature_vecs)], axis=1)\n",
    "    labeled_vecs.columns = labeled_vecs.columns.astype(str)\n",
    "    with s3.open(f\"{bucket_name}/{features_output_path}\",'wb') as f:\n",
    "        labeled_vecs.to_parquet(f)\n",
    "## Generate our training step.\n",
    "gen_features_op = comp.create_component_from_func(base_image=train_image,func=gen_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download features and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(bucket_name,features_path,model_output_path):\n",
    "    from sklearn import model_selection\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    import pandas as pd\n",
    "    import s3fs\n",
    "    import pyarrow.parquet as pq\n",
    "    from mlworkflows import util\n",
    "    ## Load features from features path\n",
    "    feats = pq.ParquetDataset(f\"s3://{bucket_name}/{features_path}\", filesystem=s3).read_pandas().to_pandas()\n",
    "    ## Train the model\n",
    "    train, test = model_selection.train_test_split(feats, random_state=43)\n",
    "    model = LogisticRegression(solver = 'lbfgs', max_iter = 4000)\n",
    "    model.fit(X=train.iloc[:,2:train.shape[1]], y=train[\"label\"])\n",
    "    # Save the model\n",
    "    with s3.open(f\"{bucket_name}/{output_path}\",'wb') as f:\n",
    "        util.serialize_to(model, f)\n",
    "## Generate our training step.\n",
    "train_op = comp.create_component_from_func(base_image=train_image,func=train_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_endpoint='https://minio-kubeflow.apps.kubeflow.openshift.red8.cloud'\n",
    "access_key='minio'\n",
    "secret_access_key='minio123'\n",
    "bucket_name='mlpipeline'\n",
    "region='us-east-1'\n",
    "\n",
    "def spam_filter_pipeline(name='spam_finder',\n",
    "                         training_data_path='data/training.parquet',\n",
    "                         features_output_path='data/features.parquet',\n",
    "                         model_output_path='data/model_output.'):\n",
    "    step1 = gen_features_op(bucket_name,training_data_path=training_data_path,features_output_path=features_output_path)\n",
    "    step2 = train_op(bucket_name,features_path=features_output_path,model_output_path=model_output_path).after(step1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/2951afd4-250e-4dc2-8ff0-441ee2295058\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/bd1b041c-80b0-4517-99c2-32b61fab5a20\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=bd1b041c-80b0-4517-99c2-32b61fab5a20)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "host=None\n",
    "\n",
    "# Submit a pipeline run\n",
    "from kfp_tekton import TektonClient\n",
    "#wow_ai_experiment = TektonClient(host=host).create_experiment(name='wow_ai', \n",
    "#                                                   description='Pipeline for reinforcement learning')\n",
    "TektonClient(host=host).create_run_from_pipeline_func(spam_filter_pipeline, experiment_name='spam_filter', arguments={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
