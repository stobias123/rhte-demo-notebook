{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/feature-engineering.png)\n",
    "\n",
    "# Finding structure\n",
    "\n",
    "We'll start by loading our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_parquet(\"data/training.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training data consists of labels (either `legitimate` or `spam`) and short documents of plausible English text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of turning real-world data into a form that a machine learning algorithm can take advantage of is called *feature engineering*.  We're going to use an approach called TF-IDF in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer,TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle, os\n",
    "\n",
    "vect = HashingVectorizer(norm=None, token_pattern='(?u)\\\\b[A-Za-z]\\\\w+\\\\b', n_features=1024, alternate_sign = False)\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "feat_pipeline = Pipeline([\n",
    "    ('vect',vect),\n",
    "    ('tfidf',tfidf)\n",
    "])\n",
    "\n",
    "from mlworkflows import util\n",
    "util.serialize_to(feat_pipeline, \"feature_pipeline.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vecs = feat_pipeline.fit_transform(data[\"text\"]).toarray()\n",
    "labeled_vecs = pd.concat([data.reset_index()[[\"index\", \"label\"]],\n",
    "                                pd.DataFrame(feature_vecs)], axis=1)\n",
    "labeled_vecs.columns = labeled_vecs.columns.astype(str)\n",
    "labeled_vecs.to_parquet(\"data/features.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "\n",
    "DIMENSIONS = 2\n",
    "\n",
    "pca2 = sklearn.decomposition.TruncatedSVD(DIMENSIONS)\n",
    "\n",
    "pca_a = pca2.fit_transform(labeled_vecs[labeled_vecs.columns[2:]].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these points to see if there's some structure in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(pca_a, columns=[\"x\", \"y\"])\n",
    "pca_df.sample(10)\n",
    "\n",
    "plot_data = pd.concat([data.reset_index(), pca_df], axis=1)\n",
    "\n",
    "import altair as alt\n",
    "alt.renderers.enable('notebook')\n",
    "\n",
    "alt.Chart(plot_data.sample(1000)).encode(x=\"x\", y=\"y\", color=\"label\").mark_point().interactive()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
